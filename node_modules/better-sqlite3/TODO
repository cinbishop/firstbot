- does BindMap need NewHandleScope in its destructor?
- maybe optimize custom function/aggregates by providing 1 and 2 argument variations
  of xFunc and xStep, to avoid memory allocation for the function arguments
  (benchmark how important this is for aggregate functions)
- enable the icu4c extension


horizontal scaling:
- multiple machine in the same local network (low ping required)
- each running a single application (nodejs app)
- that nodejs app has a custom-written "cluster"-like mechanism for communicating between processes
  - one process is "writer" process, while all other process is the "reader" process
  - depending on the http request coming in, the connection can be handled by the correct process
    (DELETE/PATCH/POST/PUT requests go to the writer process, all other requests go to a reader process)
- when a table is created, it must be specified in the application whether or not it will be a normal
  table or a sharded table (only belonging to one machine)
  - if it's a normal table, a message is sent to each other machine instructing them to create the
    table. when all machines have acknowledged that the table is created, the promise is resolved in the
    original machine's app
  - if it's a sharded table, a message is sent to each other machine notifying them that a sharded table
    is being created (and which machine it will belong to). they record that information in persistent
    storage (maybe their sqlite database itself). the machine that owns the sharded table wont send an
    acknowledgement until it has actually created the table. when all machines have sent their
    acknowledgement, the promise is resolved in the original machine's app
- every machine has a "recent transaction log" (CTL) which can probably? be stored in memory
- a "CTL check" is the process of asserting that a particular row in a normal table hasn't been UPDATED
  or DELETED within the last 1 second (configurable)
- when an UPDATE/DELETE needs to be performed on a normal table, the machine that received the request
  performs a CTL check (it must get the rowid by doing a dry-execution of the statement). if the check
  fails, the request is denied. if the check succeeds, an instruction is sent to each other machine to
  also perform a CTL check on the same data
  - machines that pass the CTL check immediately append the request information to their CTL (including
    the original/receiver machine). non-receiver machines also send their CTL check result to the
    receiver machine when their CTL check is done
  - when the receiver machine has received "success" results from each other machine, every other
    machine is notified of the success, along with the payload, and they all perform the UPDATE/DELETE operation on that rowid (including the receiver machine). no acknowledgements are necessary, and the promise is resolved on the receiver machine immediately
  - if the receiver machine received a "fail" result, every other machine is notified of the failure,
    and any machine that had sent a success deletes the entry from its CTL (including the receiver
    machine)
- when a read or write request is made containing a sharded table, it must be sent to the owner machine
  to be handled. this should be done as early as it is determined that the request requires a sharded
  table (so, by application logic)
  - requests cannot require the data of two sharded tables that belong to two different machines
    (blocking would occur while waiting for other machines to do their part of the request)
